---
title: DDIO - The journey to the datasheet and back
author: jmmichal
layout: post
identifier: ddio
---

I'm a Remote Persistent Memory guy (or RPMem for short). For the past 5 and something years we, at Intel, are breaking our heads how to make access to RPMem easy to use. We know in our bones it can change your live. Maybe not your live directly but it opens possibilites for storing data persistently and remotely which were unimaginable before. It is truly a gamechanger.

You may ask: What took you so long? A great question I hope to have a chance to answer in the coming blogposts. Why it is so hard? - hopefully it won't in the near future. [RPMA](https://github.com/pmem/rpma)... *caugh caugh*. At first glance it does not look SO hard. Especially when it was layed out initially by Chet Douglas on SNIA Storage Developer Conference (SDC) back in [2015](https://www.snia.org/sites/default/files/SDC15_presentations/persistant_mem/ChetDouglas_RDMA_with_PM.pdf). Year later the PMDK team has [released](https://github.com/pmem/pmdk/releases/tag/1.2), at least experimentaly, the library which aimed to make RPMem approachable by a wider audience. They called it suprasingly... [librpmem](https://pmem.io/pmdk/librpmem/). And... it failed badly. Why? After four years of supporting librpmem adoption and trying to help many users to make use of it, I can tell you a lot. But these are stories for another time.

Today, I want to share with you one secret. It is not exactly a secret since I will refer to materials available publicly for a long time. But sometimes the easiest way to hide a jar of sugar is put it in front of somebody's nose, and label it as 'salt', just because I can, and next to a million other mislabeled jars. I think this is how I can describe the DDIO situation.

## Data Direct IO aka Allocating Writes

Chet in his [classical SDC presentation](https://www.snia.org/sites/default/files/SDC15_presentations/persistant_mem/ChetDouglas_RDMA_with_PM.pdf) stated clearly that current HW and SW solutions do not take PMem in to account (since that time, changes are very 'gradual' so all of what he said back then still apllies in 2020). He have proposed two possible architectural solutions for RPMem:

![APM_vs_GPSPM](/assets/ddio_APM_vs_GPSPM.png)

You may notice the performance difference is roughly 2 times in favour of *Appliance Method* (APM). I think it is more then enough to at least consider using APM instead of GPSPM (*General Purpose Server Method*). The technical differences between these two are nicely depicted in Chet's presentation so I won't even try to repeat what he has said. Read the Chet's presentation! What I want to do is to dig deeper into the DDIO thing.

In the slide above words: DDIO and Non-Allocating Writes are repeated something like 10 times each. So even without trying to understand you may think it is somehow important in case of using RPMem.

[Intel Direct Data I/O Technology](https://www.intel.com/content/www/us/en/io/data-direct-i-o-technology.html) is a great stuff. When you are sending the data via network you are *probably* sending something you have processed a moment ago. So the chances are great the data is still in your CPU's caches. Similiarily, when you are obtainig the data from the network you will *probably* want to process the data as-soon-as-possible. Wouldn't it be handy, for the processing efficiency sake, to write the data directly to the CPU's caches so you won't need to wait for the data load from DRAM before processing it? This is what DDIO does. Because you have this feature turn on by default, you can do DMA transfers directly to and from CPU's caches saving up time on transfering the data between DRAM and CPU's caches.

I told you, great stuff. Is it? For 99.99...% (I realy do not know the exact number) of workloads DDIO is a blessing. RPMem is the first thing I am aware of which suffers badly because of DDIO (2 times worse performance, remember?). But there is a cure. You just have to enable *Non-Allocating Writes* on your platforms. Easy, right? The heck is *Non-Allocating Writes*? This is a mislabeled *akhem* an alternative way of saying DDIO is off. Why? When DDIO is turned on DMA Writes are consuming a required space from the CPU's caches (because you want to have the data closer to the CPU). You can say DDIO is **allocating** some of the CPU's caches for DMA Writes. So, when you turn off DDIO DMA Writes are becoming *Non-Allocating*. No more CPU's caches are allocated for DMA Writes.

I know it is confusing so I have prepared a tables with all of these labels:

|DDIO|Non-Allocating Writes|
|---|---|
|on|off|
|off|on|

Now, when we are know how the BIOS toggle is called and why we can just turn on *Non-Allocating Writes* and take the benefits of using APM (the high performance RPMem implementation)? No. Firstly, you have to deeply consider whether you really want to turn off DDIO on the whole platform. It is turned on by default for a good reason. In 99.999...% you really want to keep it turned on for the performance reasons. So, it is a hard choice. Secondly, probably your BIOS do not have *Non-Allocating Writes* toggle anyway. Why? In 99.999...% you do not want to turn it on so... why it should? This a good question to BIOS guys.

> **Note**: your BIOS probably do not come from Intel directly. So I do not even have an option to make *Non-Allocating Writes* toggle a real thing.

## CSR the saviour

Reading carefully Chet's slides deck which hanging on my wall in a gold frame (in reality it does not but you can imagine it is) you may notice he is telling us more. It is not the malicious gremlins which are using either *Allocating* or *Non-Allocating Writes*. This is the PCIe Root Port.

![HW_bg](/assets/ddio_HW_bg.png)

So, maybe it is possible to make one PCIe Root Port to benefit from DDIO blessing and at the same time another PCIe Root Port to use *Non-Allocating Writes* to allow ground breaking experience of RPMem? The answer is: yes.

> **Note** PCIe Root Port is something you won't see physically when you open yours server box. You are probably more familiar with PCIe slots where you can plug your PCIe-enabled devices. So imagine the PCIe Root Port groups few of PCIe slots (some call it PCIe Endpoints). Which one? You have to take a look into your server motherboard's manual. Having that you can pick which devices will make use *Non-Allocating Writes* (when we will manage to turn it on) and which will still benefit from DDIO.

And maybe it is possible to turn on *Non-Allocating Writes* using *Configuration Space Register* (CSR)? What da heck? I know it is a huge leap. Let's assume I have found it in a bag of chips ;-) CSRs allow to control and read status of various devices in the system. In my bag of chips I have found out CSRs also allows controling the behaviour of PCIe Root Ports including use of *Non-Allocating Writes*.

## Finding a way out of a Datasheet

I have found for you [Intel&copy; Xeon&copy; Processor Scalable Family Datasheet, Volume Two: Registers](https://www.intel.com/content/dam/www/public/us/en/documents/datasheets/xeon-scalable-datasheet-vol-1.pdf). So now you know everything. Just kidding. Keep calm and we will go together through it.

Let's make use of ```Ctrl + F``` and type "non-allocating". I have found three occurances of the word where all of them happend to be located in a register called **PERFCTRLSTS_0** (registers are not known from having pretty names). Two of those occurances are called **ForceNoSnoopWrEn** and **NoSnoopOpWrEn**. Both of these are regarding non-snoop writes which is applicable to memory regions for which CPU cacheing is prohibited so they are "*non-allocating*" by definiton.

![HW_bg](/assets/ddio_perfctrlsts_0.png)

The last one which is important in this topic is **Use_Allocating_Flow_Wr**. It is by default set to ```1b``` which means all snooping writes are *Allocating Writes*. The documentation promise when we manage to change this bit to ```0b``` all snooping writes will be *Non-Allocating*. This is what we aim.

## Having one 0

Pick your sword... I meant a tool. Linux distributions comes with variety of useful small tools. One of them is [setpci(8)](https://linux.die.net/man/8/setpci) which is "a utility for querying and configuring PCI devices". Great!

After studying the manual carfully you may come with the following command sequence:

```sh
# as root
$ setpci -s $PCIe_Root_Port 180.b
91
$ setpci -s $PCIe_Root_Port 180.b=11
$ setpci -s $PCIe_Root_Port 180.b
11
```

Where 91 is a hexadecimal equivalent of the default values mentioned in the datasheet we have already read (if you obtain other value you will have to proceed accordingly). The ```0x91 == 1001 0001b```. The 7th bit of the value, which is **Use_Allocating_Flow_Wr** is equal to ```1b```. After setting it to ```0b``` the value will be ```0001 0001b == 0x11```. This is where the ```11``` comes from.

The last call to ```setpci``` is a simple verification if the change we meant to do has been taken into consideration. *Non-Allocating Writes* are turned on. The DDIO is turned off. We are done!

## Appendix P - How to find a PCIe Root Port?

You might notice in the code snippet above I have used a variable ```$PCIe_Root_Port```. You may feel a little bit unsatisfied and the code above well... it won't work without it. So let's fix it.

It is hard to show a general way to obtain an address of the PCIe Root Port you desire. Let's assume you have a Mellanox RDMA-capable Network Adapter plugged into PCIe slot. How to find out an address of PCIe Root Port of the Mellanox RNIC? If you ask me I will argue the simplest way is as follows:

```sh
$ lspci -vt | grep Mellanox
 +-[0000:17]-+-00.0-[18]--+-00.0  Mellanox Technologies MT27800 Family [ConnectX-5]
 |           |            \-00.1  Mellanox Technologies MT27800 Family [ConnectX-5]
```

The lspci(3) manual pages will tell you that ```-t``` show a tree-like diagram containing all buses, bridges, devices and connections between them whereas ```-v``` display detailed information about all devices. So we end up with a tree like structure of named devices which allows us track down the PCIe Root Port and its address. In this case it should be written as ```0000:17:00.0```. That's all Folks!

## Summary

RPMem can give you an impossible, to this time, performance boost in case you are into writing data between server nodes in a persistent way. But it requires from you to prepare your platform to do so.

RPMem works the best when it makes use of the Appliance Method (thank you Chet!). But the Appliance Method on todays platform (having ADR - so no persistent CPU caches) requires turning off the Direct Data I/O technology. So all writes will go stright to the PMem without being cached by the CPU.

You can turn off the DDIO at the PCIe Root Port but it requires magic... *ekhem* some know-how. Which was layed down for you in this article. All of this boils down to the following commands sequence:

```sh
# as root

# 1. identify an address of the PCIe Root Port
$ export PCIe_Root_Port=0000:17:00.0

# 2. read the current value of the PERFCTRLSTS_0 register
$ setpci -s $PCIe_Root_Port 180.b
91

# 3. turn off the Use_Allocating_Flow_Wr bit
$ setpci -s $PCIe_Root_Port 180.b=11

# 4. verify the result
$ setpci -s $PCIe_Root_Port 180.b
11
```

Happy RPMeming! :-)