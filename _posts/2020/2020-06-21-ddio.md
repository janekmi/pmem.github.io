---
title: DDIO: The journey to the datasheet and back
author: jmmichal
layout: post
identifier: ddio
---

I'm a Remote Persistent Memory guy (or RPMem for short). For the past 5 and something years we, at Intel, are breaking our heads how to make access to RPMem easy to use. We know in our bones it can change your live. Maybe not your live directly but it opens possibilites for storing data persistently and remotely which were unimaginable before. It is truly a gamechanger.

You may ask: What took you so long? A great question I hope to have a chance to answer in the coming blogposts. Why it is so hard? - hopefully it won't in the near future. [RPMA](https://github.com/pmem/rpma)... *caugh caugh*. At first glance it does not look SO hard. Especially when it was layed out initially by Chet Douglas on SNIA Storage Developer Conference (SDC) back in [2015](https://www.snia.org/sites/default/files/SDC15_presentations/persistant_mem/ChetDouglas_RDMA_with_PM.pdf). Year later the PMDK team has [released](https://github.com/pmem/pmdk/releases/tag/1.2), at least experimentaly, the library which aimed to make RPMem approachable by a wider audience. They called it suprasingly... [librpmem](https://pmem.io/pmdk/librpmem/). And... it failed badly. Why? After four years of supporting librpmem adoption and trying to help many users to make use of it, I can tell you a lot. But these are stories for another time.

Today, I want to share with you one secret. It is not exactly a secret since I will refer to materials available publicly for a long time. But sometimes the easiest way to hide a jar of sugar is put it in front of somebody's nose, and label it as 'salt', just because I can, and next to a million other mislabeled jars. I think this is how I can describe the DDIO situation.

## Data Direct IO aka Allocating Writes

Chet in his [classical SDC presentation](https://www.snia.org/sites/default/files/SDC15_presentations/persistant_mem/ChetDouglas_RDMA_with_PM.pdf) stated clearly that current HW and SW solutions do not take PMem in to account (since that time, changes are very 'gradual' so all of what he said back then still apllies in 2020). He have proposed two possible architectural solutions for RPMem:

![APM_vs_GPSPM](/assets/ddio_APM_vs_GPSPM.png)

You may notice the performance difference is roughly 2 times in favour of *Appliance Method* (APM). I think it is more then enough to at least consider using APM instead of GPSPM (*General Purpose Server Method*). The technical differences between these two are nicely depicted in Chet's presentation so I won't even try to repeat what he has said. Read the Chet's presentation! What I want to do is to dig deeper into the DDIO thing.

In the slide above words: DDIO and Non-Allocating Writes are repeated something like 10 times each. So even without trying to understand you may think it is somehow important in case of using RPMem.

[Intel Direct Data I/O Technology](https://www.intel.com/content/www/us/en/io/data-direct-i-o-technology.html) is a great stuff. When you are sending the data via network you are *probably* sending something you have processed a moment ago. So the chances are great the data is still in your CPU's caches. Similiarily, when you are obtainig the data from the network you will *probably* want to process the data as-soon-as-possible. Wouldn't it be handy, for the processing efficiency sake, to write the data directly to the CPU's caches so you won't need to wait for the data load from DRAM before processing it? This is what DDIO does. Because you have this feature turn on by default, you can do DMA transfers directly to and from CPU's caches saving up time on transfering the data between DRAM and CPU's caches.

I told you, great stuff. Is it? For 99.99...% (I realy do not know the exact number) of workloads DDIO is a blessing. RPMem is the first thing I am aware of which suffers badly because of DDIO (2 times worse performance, remember?). But there is a cure. You just have to enable *Non-Allocating Writes* on your platforms. Easy, right? The heck is *Non-Allocating Writes*? This is a mislabeled *akhem* an alternative way of saying DDIO is off. Why? When DDIO is turned on DMA Writes are consuming a required space from the CPU's caches (because you want to have the data closer to the CPU). You can say DDIO is **allocating** some of the CPU's caches for DMA Writes. So, when you turn off DDIO DMA Writes are becoming *Non-Allocating*. No more CPU's caches are allocated for DMA Writes.

I know it is confusing so I have prepared a tables with all of these labels:

|DDIO|Non-Allocating Writes|
|---|---|
|on|off|
|off|on|

Now, when we are know how the BIOS toggle is called and why we can just turn on *Non-Allocating Writes* and take the benefits of using APM (the high performance RPMem implementation)? No. Firstly, you have to deeply consider whether you really want to turn off DDIO on the whole platform. It is turned on by default for a good reason. In 99.999...% you really want to keep it turned on for the performance reasons. So, it is a hard choice. Secondly, probably your BIOS do not have *Non-Allocating Writes* toggle anyway. Why? In 99.999...% you do not want to turn it on so... why it should? This a good question to BIOS guys.

> **Note**: your BIOS probably do not come from Intel directly. So I do not even have an option to make *Non-Allocating Writes* toggle a real thing.

## CSR the saviour

Reading carefully Chet's slides deck which hanging on my wall in a gold frame (in reality it does not but you can imagine it is) you may notice he is telling us more. It is not the malicious gremlins which are using either *Allocating* or *Non-Allocating Writes*. This is the PCIe Root Port.

![HW_bg](/assets/ddio_HW_bg.png)

So, maybe it is possible to make one PCIe Root Port to benefit from DDIO blessing and at the same time another PCIe Root Port to use *Non-Allocating Writes* to allow ground breaking experience of RPMem? The answer is: yes.

> **Note** PCIe Root Port is something you won't see physically when you open yours server box. You are probably more familiar with PCIe slots where you can plug your PCIe-enabled devices. So imagine the PCIe Root Port groups few of PCIe slots (some call it PCIe Endpoints). Which one? You have to take a look into your server motherboard's manual. Having that you can pick which devices will make use *Non-Allocating Writes* (when we will manage to turn it on) and which will still benefit from DDIO.

And maybe it is possible to turn on *Non-Allocating Writes* using *Configuration Space Register* (CSR)? What da heck? I know it is a huge leap. Let's assume I have found it in a bag of chips ;-) CSRs allow to control and read status of various devices in the system. In my bag of chips I have found out CSRs also allows controling the behaviour of PCIe Root Ports including use of *Non-Allocating Writes*.

## Finding a way out of a Datasheet

I have found for you [Intel&copy; Xeon&copy; Processor Scalable Family Datasheet, Volume Two: Registers](https://www.intel.com/content/dam/www/public/us/en/documents/datasheets/xeon-scalable-datasheet-vol-1.pdf). So now you know everything. Just kidding. Keep calm and we will go together through it.

Let's make use of ```Ctrl + F``` and type "non-allocating". I have found three occurances of the word where all of them happend to be located in a register called **PERFCTRLSTS_0** (registers are not known from having pretty names). Two of those occurances are called **ForceNoSnoopWrEn** and **NoSnoopOpWrEn**. Both of these are regarding non-snoop writes which is applicable to memory regions for which CPU cacheing is prohibited so they are "*non-allocating*" by definiton.

![HW_bg](/assets/ddio_perfctrlsts_0.png)

The last one which is important in this topic is **Use_Allocating_Flow_Wr**. It is by default set to ```1b``` which means all snooping writes are *Allocating Writes*. The documentation promise when we manage to change this bit to ```0b``` all snooping writes will be *Non-Allocating*. This is what we aim.

## Having one 0

XXX

## Summary

XXX